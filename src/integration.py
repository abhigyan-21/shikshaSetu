"""
Integration module that wires all components together for end-to-end pipeline testing.

This module connects:
- Content Pipeline Orchestrator
- API endpoints (Flask/FastAPI)
- Frontend integration
- Content Repository
- All pipeline components (simplifier, translator, validator, speech generator)
"""

import os
import logging
from typing import Dict, Any, Optional
from uuid import UUID

from .pipeline.orchestrator import ContentPipelineOrchestrator
from .repository.content_repository import ContentRepository
from .repository.database import get_db
from .monitoring.metrics_collector import MetricsCollector

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class IntegratedPipeline:
    """
    Integrated pipeline that connects all components for end-to-end processing.
    
    This class provides a unified interface for:
    - Processing content through the full pipeline
    - Storing results in the repository
    - Tracking metrics and performance
    - Retrieving processed content
    """
    
    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize the integrated pipeline with all components.
        
        Args:
            api_key: Optional Hugging Face API key
        """
        logger.info("Initializing integrated pipeline...")
        
        # Initialize database
        db = get_db()
        db.create_tables()
        
        # Initialize core components
        self.orchestrator = ContentPipelineOrchestrator(api_key=api_key)
        self.repository = ContentRepository()
        self.metrics_collector = MetricsCollector()
        
        logger.info("Integrated pipeline initialized successfully")
    
    def process_and_store(
        self,
        input_data: str,
        target_language: str,
        grade_level: int,
        subject: str,
        output_format: str = 'both'
    ) -> Dict[str, Any]:
        """
        Process content through the pipeline and store in repository.
        
        This is the main end-to-end flow:
        1. Validate input parameters
        2. Process through pipeline (simplification → translation → validation → speech)
        3. Store in repository
        4. Track metrics
        5. Return complete result
        
        Args:
            input_data: Raw educational content
            target_language: Target Indian language
            grade_level: Grade level (5-12)
            subject: Subject area
            output_format: Output format ('text', 'audio', 'both')
        
        Returns:
            Dictionary with processed content and metadata
        """
        logger.info(f"Starting end-to-end processing: language={target_language}, grade={grade_level}, subject={subject}")
        
        try:
            # Step 1: Process through pipeline
            result = self.orchestrator.process_content(
                input_data=input_data,
                target_language=target_language,
                grade_level=grade_level,
                subject=subject,
                output_format=output_format
            )
            
            # Step 2: Store in repository (already done by orchestrator, but verify)
            content = self.repository.retrieve(UUID(result.id))
            
            if not content:
                logger.warning("Content not found in repository after processing, storing now...")
                content = self.repository.store(
                    original_text=result.original_text,
                    simplified_text=result.simplified_text,
                    translated_text=result.translated_text,
                    language=result.language,
                    grade_level=result.grade_level,
                    subject=result.subject,
                    audio_file_path=result.audio_file_path,
                    ncert_alignment_score=result.ncert_alignment_score,
                    audio_accuracy_score=result.audio_accuracy_score,
                    metadata=result.metadata
                )
            
            # Step 3: Track metrics
            self.metrics_collector.record_pipeline_execution(
                content_id=result.id,
                language=result.language,
                grade_level=result.grade_level,
                subject=result.subject,
                processing_time_ms=result.metadata.get('total_processing_time_ms', 0),
                success=True,
                ncert_score=result.ncert_alignment_score,
                audio_score=result.audio_accuracy_score
            )
            
            # Step 4: Build comprehensive response
            response = {
                'success': True,
                'content_id': result.id,
                'content': {
                    'original_text': result.original_text,
                    'simplified_text': result.simplified_text,
                    'translated_text': result.translated_text,
                    'language': result.language,
                    'grade_level': result.grade_level,
                    'subject': result.subject,
                    'audio_file_path': result.audio_file_path,
                    'audio_url': f"/api/content/{result.id}/audio" if result.audio_file_path else None
                },
                'quality_scores': {
                    'ncert_alignment_score': result.ncert_alignment_score,
                    'audio_accuracy_score': result.audio_accuracy_score,
                    'validation_status': result.validation_status
                },
                'metrics': {
                    'total_processing_time_ms': result.metadata.get('total_processing_time_ms', 0),
                    'stage_metrics': [
                        {
                            'stage': m.stage,
                            'processing_time_ms': m.processing_time_ms,
                            'success': m.success,
                            'retry_count': m.retry_count
                        }
                        for m in result.metrics
                    ]
                },
                'metadata': result.metadata
            }
            
            logger.info(f"End-to-end processing completed successfully: content_id={result.id}")
            
            return response
            
        except Exception as e:
            logger.error(f"End-to-end processing failed: {str(e)}", exc_info=True)
            
            # Track failure metrics
            self.metrics_collector.record_pipeline_execution(
                content_id=None,
                language=target_language,
                grade_level=grade_level,
                subject=subject,
                processing_time_ms=0,
                success=False,
                error_message=str(e)
            )
            
            raise
    
    def retrieve_content(
        self,
        content_id: str,
        use_cache: bool = True
    ) -> Optional[Dict[str, Any]]:
        """
        Retrieve processed content from repository.
        
        Args:
            content_id: UUID of the content
            use_cache: Whether to use offline cache
        
        Returns:
            Dictionary with content data or None if not found
        """
        try:
            content = self.repository.retrieve(UUID(content_id), use_cache=use_cache)
            
            if not content:
                return None
            
            return {
                'id': str(content.id),
                'original_text': content.original_text,
                'simplified_text': content.simplified_text,
                'translated_text': content.translated_text,
                'language': content.language,
                'grade_level': content.grade_level,
                'subject': content.subject,
                'audio_file_path': content.audio_file_path,
                'audio_url': f"/api/content/{content.id}/audio" if content.audio_file_path else None,
                'ncert_alignment_score': content.ncert_alignment_score,
                'audio_accuracy_score': content.audio_accuracy_score,
                'created_at': content.created_at.isoformat() if content.created_at else None,
                'metadata': content.content_metadata
            }
            
        except Exception as e:
            logger.error(f"Failed to retrieve content {content_id}: {str(e)}")
            return None
    
    def search_content(
        self,
        language: Optional[str] = None,
        grade_level: Optional[int] = None,
        subject: Optional[str] = None,
        limit: int = 50
    ) -> Dict[str, Any]:
        """
        Search for content with filters.
        
        Args:
            language: Filter by language
            grade_level: Filter by grade level
            subject: Filter by subject
            limit: Maximum results
        
        Returns:
            Dictionary with search results
        """
        try:
            contents = self.repository.retrieve_by_filters(
                language=language,
                grade_level=grade_level,
                subject=subject,
                limit=limit
            )
            
            results = []
            for content in contents:
                results.append({
                    'id': str(content.id),
                    'language': content.language,
                    'grade_level': content.grade_level,
                    'subject': content.subject,
                    'translated_text_preview': content.translated_text[:200] + '...' if len(content.translated_text) > 200 else content.translated_text,
                    'ncert_alignment_score': content.ncert_alignment_score,
                    'audio_available': content.audio_file_path is not None,
                    'created_at': content.created_at.isoformat() if content.created_at else None
                })
            
            return {
                'total_count': len(results),
                'results': results,
                'filters': {
                    'language': language,
                    'grade_level': grade_level,
                    'subject': subject
                }
            }
            
        except Exception as e:
            logger.error(f"Search failed: {str(e)}")
            return {'total_count': 0, 'results': [], 'error': str(e)}
    
    def create_offline_package(
        self,
        content_ids: list,
        package_name: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Create offline package for batch download.
        
        Args:
            content_ids: List of content IDs
            package_name: Optional package name
        
        Returns:
            Dictionary with package information
        """
        try:
            # Convert string IDs to UUIDs
            uuids = [UUID(cid) for cid in content_ids]
            
            # Create package
            package_path = self.repository.batch_download(
                content_ids=uuids,
                package_name=package_name
            )
            
            # Get package size
            import os
            package_size = os.path.getsize(package_path)
            
            return {
                'success': True,
                'package_path': package_path,
                'package_size_bytes': package_size,
                'package_size_mb': round(package_size / (1024 * 1024), 2),
                'content_count': len(content_ids)
            }
            
        except Exception as e:
            logger.error(f"Failed to create offline package: {str(e)}")
            return {'success': False, 'error': str(e)}
    
    def get_system_health(self) -> Dict[str, Any]:
        """
        Get system health status and metrics.
        
        Returns:
            Dictionary with system health information
        """
        try:
            # Get metrics summary
            metrics_summary = self.metrics_collector.get_summary()
            
            # Get cache statistics
            cache_stats = self.repository.get_cache_size()
            
            # Check database connection
            db_healthy = True
            try:
                session = get_db().get_session()
                session.execute("SELECT 1")
                session.close()
            except Exception:
                db_healthy = False
            
            return {
                'status': 'healthy' if db_healthy else 'degraded',
                'database': {
                    'connected': db_healthy
                },
                'metrics': metrics_summary,
                'cache': cache_stats,
                'components': {
                    'orchestrator': 'operational',
                    'repository': 'operational',
                    'metrics_collector': 'operational'
                }
            }
            
        except Exception as e:
            logger.error(f"Health check failed: {str(e)}")
            return {
                'status': 'unhealthy',
                'error': str(e)
            }


# Global instance for API usage
_integrated_pipeline = None


def get_integrated_pipeline(api_key: Optional[str] = None) -> IntegratedPipeline:
    """
    Get or create the global integrated pipeline instance.
    
    Args:
        api_key: Optional Hugging Face API key
    
    Returns:
        IntegratedPipeline instance
    """
    global _integrated_pipeline
    
    if _integrated_pipeline is None:
        _integrated_pipeline = IntegratedPipeline(api_key=api_key)
    
    return _integrated_pipeline


def test_end_to_end_flow(
    sample_text: str = "Photosynthesis is the process by which plants convert sunlight into energy.",
    target_language: str = "Hindi",
    grade_level: int = 8,
    subject: str = "Science"
) -> Dict[str, Any]:
    """
    Test the complete end-to-end pipeline flow.
    
    This function demonstrates the full integration:
    1. Input → Simplification → Translation → Validation → Speech → Storage → Retrieval
    
    Args:
        sample_text: Sample educational content
        target_language: Target language
        grade_level: Grade level
        subject: Subject area
    
    Returns:
        Dictionary with test results
    """
    logger.info("=" * 80)
    logger.info("STARTING END-TO-END INTEGRATION TEST")
    logger.info("=" * 80)
    
    pipeline = get_integrated_pipeline()
    
    try:
        # Step 1: Process content
        logger.info("\n[STEP 1] Processing content through pipeline...")
        result = pipeline.process_and_store(
            input_data=sample_text,
            target_language=target_language,
            grade_level=grade_level,
            subject=subject,
            output_format='both'
        )
        
        content_id = result['content_id']
        logger.info(f"✓ Content processed successfully: ID={content_id}")
        logger.info(f"  - NCERT Alignment: {result['quality_scores']['ncert_alignment_score']:.2%}")
        logger.info(f"  - Audio Accuracy: {result['quality_scores'].get('audio_accuracy_score', 0):.2%}")
        logger.info(f"  - Processing Time: {result['metrics']['total_processing_time_ms']}ms")
        
        # Step 2: Retrieve content
        logger.info("\n[STEP 2] Retrieving content from repository...")
        retrieved = pipeline.retrieve_content(content_id)
        
        if retrieved:
            logger.info(f"✓ Content retrieved successfully")
            logger.info(f"  - Language: {retrieved['language']}")
            logger.info(f"  - Grade: {retrieved['grade_level']}")
            logger.info(f"  - Subject: {retrieved['subject']}")
        else:
            logger.error("✗ Failed to retrieve content")
            return {'success': False, 'error': 'Content retrieval failed'}
        
        # Step 3: Search for content
        logger.info("\n[STEP 3] Searching for content...")
        search_results = pipeline.search_content(
            language=target_language,
            grade_level=grade_level,
            subject=subject
        )
        
        logger.info(f"✓ Search completed: {search_results['total_count']} results found")
        
        # Step 4: Create offline package
        logger.info("\n[STEP 4] Creating offline package...")
        package_result = pipeline.create_offline_package(
            content_ids=[content_id],
            package_name="test_package"
        )
        
        if package_result['success']:
            logger.info(f"✓ Offline package created: {package_result['package_size_mb']}MB")
        else:
            logger.warning(f"⚠ Package creation failed: {package_result.get('error')}")
        
        # Step 5: Check system health
        logger.info("\n[STEP 5] Checking system health...")
        health = pipeline.get_system_health()
        logger.info(f"✓ System status: {health['status']}")
        
        logger.info("\n" + "=" * 80)
        logger.info("END-TO-END INTEGRATION TEST COMPLETED SUCCESSFULLY")
        logger.info("=" * 80)
        
        return {
            'success': True,
            'content_id': content_id,
            'processing_result': result,
            'retrieval_result': retrieved,
            'search_result': search_results,
            'package_result': package_result,
            'health_check': health
        }
        
    except Exception as e:
        logger.error("\n" + "=" * 80)
        logger.error(f"END-TO-END INTEGRATION TEST FAILED: {str(e)}")
        logger.error("=" * 80)
        
        return {
            'success': False,
            'error': str(e)
        }


if __name__ == '__main__':
    # Run end-to-end test when module is executed directly
    test_result = test_end_to_end_flow()
    
    if test_result['success']:
        print("\n✓ All integration tests passed!")
        print(f"\nTest Content ID: {test_result['content_id']}")
    else:
        print(f"\n✗ Integration test failed: {test_result['error']}")
